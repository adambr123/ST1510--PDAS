{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDAS CA2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def load_csv_files(file_names):\n",
    "    dataframes = []\n",
    "    for file_names in file_names:\n",
    "        df = pd.read_csv(file_names)\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "file_names = ['./greenhousepollution.csv', './solidwastemanagement.csv', './LicensedFoodEstablishmentsbyCategoryAnnual.csv']\n",
    "dataframes = load_csv_files(file_names)\n",
    "\n",
    "\n",
    "df1 = dataframes[0]\n",
    "df2 = dataframes[1]\n",
    "df3 = dataframes[2]\n",
    "\n",
    "# nature of datasets\n",
    "print(\"Summary of Greenhouse gas pollution\")\n",
    "print(df1.info())\n",
    "print(df1.head())\n",
    "\n",
    "print(\"Summary of Solid Waste Management\")\n",
    "print(df2.info())\n",
    "print(df2.head())\n",
    "\n",
    "print(\"Summary of Total Licencensed Food Establishment\")\n",
    "print(df3.info())\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary of Greenhouse gas pollution, we can see that the DataFrame has 22 entries (rows) and 16 columns. The columns represent different types of greenhouse gases and their emissions measured in metric tons of CO2 equivalent. The ‘Year’ column is of integer type (int64), while the rest of the columns are of float type (float64).\n",
    "we can also see some values in the DataFrame that have missing values (NaN), as indicated by the count of non-null entries being less than 22 for some columns. Therefore, we need to create a function to handle missing values.\n",
    "\n",
    "From the summary of Solid Waste Management, we can see that the DataFrame provides information about each column, such as the count of non-null entries, the number of unique entries, the most frequent entry (top), and the frequency of the most frequent entry (freq).\n",
    "From the summary, we can see that the DataFrame has 11 entries (rows) and 61 columns. The columns represent different types of waste and their quantities generated and recycled over the years. The ‘Year’ column is of integer type (int64), while the rest of the columns are of float type (float64).\n",
    "we can also see some values in the DataFrame that have missing values (NaN), as indicated by the count of non-null entries being less than 11 for some columns. Therefore, we need to create a function to handle missing values.\n",
    "\n",
    "From the summary of Total Licensed Food Establishment, We can see that he DataFrame has 25 entries (rows) and 3 columns: ‘year’, ‘level_1’, and ‘Total Licensed Food Establishments’.\n",
    "The data types for these columns are int64, object, and float64 respectively.\n",
    "The ‘year’ column contains integer values representing the year.\n",
    "The ‘level_1’ column seems to contain the string ‘Total Licensed Food Establishments’ for all rows.\n",
    "The ‘Total Licensed Food Establishments’ column contains float values, which likely represent the number of licensed food establishments for each year. There are 19 non-null entries in this column, suggesting that there may be some missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing values\n",
    "def handle_missing_values(dataframes):\n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Check for null values\n",
    "        print(f\"\\nBefore imputation for dataframe {i+1}: \")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "\n",
    "        # Fill missing values using the specified method\n",
    "        df_filled = df.copy()\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':  # Categorical data\n",
    "                df_filled[column].fillna(df[column].mode()[0], inplace=True)\n",
    "            else:  # Numeric data\n",
    "                df_filled[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "        # Check for missing values after imputation to show whether the datasets has been imputed\n",
    "        print(f\"\\nAfter imputation for dataframe {i+1}: \")\n",
    "        print(df_filled.isnull().sum())\n",
    "\n",
    "        # Replace the original dataframe in the list with the filled dataframe\n",
    "        dataframes[i] = df_filled\n",
    "\n",
    "    # Return the list of filled dataframes\n",
    "    return dataframes\n",
    "\n",
    "dataframes = handle_missing_values(dataframes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Detecting missing values\n",
    "  imputed missing values \n",
    "- Detect outliers\n",
    "  - Outliers are detected by using the 1.5*IQR (interquartile range) rule\n",
    "  - Due to the large number of outliers, the outliers are not removed as it will affect the accuracy of the model\n",
    "- Merge datasets\n",
    "- Print head of the dataframe (sample data)\n",
    "- Export dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_remove_outliers(dataframes):\n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Select numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            # Calculate number of outliers\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))].shape[0]\n",
    "            print(f'Number of outliers in {col} in dataframe {i+1}: {outliers}\\n')\n",
    "\n",
    "            # Remove outliers\n",
    "            df = df[~((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))]\n",
    "            \n",
    "        # Replace the original dataframe in the list with the outlier-removed dataframe\n",
    "        dataframes[i] = df\n",
    "\n",
    "    # Return the list of outlier-removed dataframes\n",
    "    return dataframes\n",
    "\n",
    "# Call the function\n",
    "dataframes = calculate_and_remove_outliers(dataframes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal\n",
    "Since the Number of Outliers in each column is generally small, ive decided to just remove the outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the imputed and outlier-removed dataframes\n",
    "df1_imputed = dataframes[0]\n",
    "df2_imputed = dataframes[1]\n",
    "df3_imputed = dataframes[2]\n",
    "\n",
    "# Concatenate dataframes horizontally (add columns)\n",
    "merged_df = pd.concat([df1_imputed.set_index('Year'), df2_imputed.set_index('Year'), df3_imputed.set_index('Year')], axis=1)\n",
    "\n",
    "# Define independent variable\n",
    "X = merged_df['Total Licensed Food Establishments']\n",
    "\n",
    "\n",
    "# Add a constant to the independent value\n",
    "X1 = sm.add_constant(X)\n",
    "\n",
    "# Replace inf with nan and drop rows with nan values\n",
    "X1.replace([np.inf, -np.inf], np.nan, inplace=True)  \n",
    "X1.dropna(inplace=True)\n",
    "\n",
    "# Align indices of X1 with merged_df\n",
    "aligned_index = X1.index.intersection(merged_df.index)\n",
    "X1 = X1.loc[aligned_index]\n",
    "merged_df = merged_df.loc[aligned_index]\n",
    "\n",
    "# Define dependent variables\n",
    "y1 = merged_df['  Carbon Dioxide (CO2) (Mt CO2-Equivalent)']\n",
    "y2 = merged_df['Total Greenhouse Gas Emissions (Mt CO2-Equivalent)']\n",
    "\n",
    "# Impute missing values in the dependent variables\n",
    "y1.fillna(y1.mean(), inplace=True)\n",
    "y2.fillna(y2.mean(), inplace=True)\n",
    "\n",
    "# Check for constant values\n",
    "if y1.nunique() == 1 or y2.nunique() == 1:\n",
    "    print(\"Dependent variables have constant values. Please select different variables.\")\n",
    "else:\n",
    "    # Apply linear regression models\n",
    "    model1 = sm.OLS(y1, X1)\n",
    "    results1 = model1.fit()\n",
    "    print(results1.summary())\n",
    "\n",
    "    model2 = sm.OLS(y2, X1)\n",
    "    results2 = model2.fit()\n",
    "    print(results2.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Total Licensed Food Establishments being the indepedent variable and Total Greenhouse Gas Emissions (Mt CO2-Equivalent) being the dependent the model summary results are that the R-squared value is 0.068, which means that only about 6.8% of the variance in the dependent variable (‘Total Greenhouse Gas Emissions’) can be explained by the independent variable (‘Total Licensed Food Establishments’). This is quite low, suggesting that the model does not explain much of the variability in the data.\n",
    "The adjusted R-squared value is 0.022, which is even lower. The adjusted R-squared takes into account the number of predictors in the model (in this case, just one), and can sometimes be a better indicator of the quality of a model.\n",
    "The F-statistic is 1.471 and the Prob (F-statistic) is 0.239. The F-statistic is a measure of how much better the model fits the data than a model with no independent variables. The Prob (F-statistic) is the probability of getting an F-statistic as extreme as the one calculated if the null hypothesis is true (i.e., if the model with no independent variables fits the data as well as your model). A Prob (F-statistic) less than 0.05 is often considered statistically significant, but in this case, it’s 0.239, which is not less than 0.05.\n",
    "Based on these statistics, it seems that the model might not be a good fit for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new DataFrame with the variables of interest\n",
    "df = pd.DataFrame({\n",
    "    'Total Generated Waste': df2_imputed['Total Generated (Tonnes)'],\n",
    "    'Total Food Establishments': df3_imputed['Total Licensed Food Establishments'],\n",
    "    'Total Greenhouse Gas Emissions': df1_imputed['Total Greenhouse Gas Emissions (Mt CO2-Equivalent)']\n",
    "})\n",
    "\n",
    "\n",
    "# Plot QQ plots for each column\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sm.qqplot(df[column], line='s')\n",
    "    plt.title(f'QQ Plot for {column}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# Method 1: Scatter plot matrix\n",
    "sns.pairplot(df)\n",
    "plt.title('Pairplot of Total Generated Waste, Total Food Establishments, and Total Greenhouse Gas Emissions')\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Plotting scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Total Food Establishments'], df['Total Greenhouse Gas Emissions'], alpha=0.5)\n",
    "plt.title('Scatter Plot of Total Food Establishments vs Total Greenhouse Gas Emissions')\n",
    "plt.xlabel('Total Food Establishments')\n",
    "plt.ylabel('Total Greenhouse Gas Emissions (Mt CO2-Equivalent)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting jointplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.jointplot(data=df, x='Total Food Establishments', y='Total Greenhouse Gas Emissions', kind='reg', color='purple')\n",
    "plt.xlabel('Total Food Establishments')\n",
    "plt.ylabel('Total Greenhouse Gas Emissions (Mt CO2-Equivalent)')\n",
    "plt.title('Jointplot of Total Food Establishments vs Total Greenhouse Gas Emissions')\n",
    "plt.show()\n",
    "\n",
    "# Create a linear regression plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(data=df, x='Total Food Establishments', y='Total Greenhouse Gas Emissions', scatter_kws={'s': 50}, line_kws={'color': 'red'})\n",
    "plt.xlabel('Total Food Establishments')\n",
    "plt.ylabel('Total Greenhouse Gas Emissions (Mt CO2-Equivalent)')\n",
    "plt.title('Linear Regression: Total Food Establishments vs Total Greenhouse Gas Emissions')\n",
    "plt.show()\n",
    "\n",
    "# Method 5: Jointplot\n",
    "sns.jointplot(x='Total Generated Waste', y='Total Food Establishments', data=df, kind='reg')\n",
    "plt.xlabel('Total Generated Waste')\n",
    "plt.ylabel('Total Food Establishments')\n",
    "plt.title('Jointplot: Total Generated Waste vs Total Food Establishments')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
